{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"mode.copy_on_write\", True)\n",
    "\n",
    "script_dir = os.path.abspath(\"\")\n",
    "mymodule_dir = os.path.join(script_dir, \"..\", \"scripts\")\n",
    "sys.path.append(mymodule_dir)\n",
    "import cleaning\n",
    "\n",
    "jobs = cleaning.prep(\n",
    "    pd.read_pickle(\"../data/jobs.pkl\"), filtered=False, categories_reduced=False\n",
    ")\n",
    "# jobs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = jobs[\n",
    "    (jobs[\"Description\"] != \"Nicht stepstone\")\n",
    "    & (jobs[\"Description\"] != \"Stellenanzeige nicht mehr verfügbar\")\n",
    "    & (jobs[\"Description\"] != \"\")\n",
    "]\n",
    "texts = jobs[\"Description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/tim/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from HanTa import HanoverTagger as ht\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_doc(doc):\n",
    "    stopwords_ger = stopwords.words(\"german\")\n",
    "    stopwords_ger.extend([\"du\", \"sie\"])\n",
    "    stopwords_en = stopwords.words(\"english\")\n",
    "\n",
    "    cleaned = re.sub(\n",
    "        r\"[\\W](?=\\w)\",\n",
    "        \" \",\n",
    "        doc.replace(\"\\u200b\", \" \")\n",
    "        .replace(\"\\xad\", \"\")\n",
    "        .replace(\"-\", \" \")\n",
    "        .replace(\"/\", \" \")\n",
    "        .replace(\"·\", \" \")\n",
    "        .replace(\"•\", \" \")\n",
    "        .replace(\"…\", \" \"),\n",
    "    )\n",
    "    tokenized = word_tokenize(cleaned, language=\"german\")\n",
    "    alphas = [w.lower() for w in tokenized if w.isalpha()]\n",
    "\n",
    "    tagger = ht.HanoverTagger(\"morphmodel_ger.pgz\")\n",
    "\n",
    "    no_stops_de = [w for w in alphas if w not in stopwords_ger]\n",
    "    no_stops_de_en = [w for w in no_stops_de if w not in stopwords_en]\n",
    "\n",
    "    lemmatized = [tagger.analyze(token)[0].lower() for token in no_stops_de_en]\n",
    "\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(\"../data/lemmatized_desc.pkl\"):\n",
    "    with open(\"../data/lemmatized_desc.pkl\", \"rb\") as file:\n",
    "        old_lemmas = pickle.load(file)\n",
    "else:\n",
    "    old_lemmas = []\n",
    "\n",
    "lemmas = old_lemmas\n",
    "\n",
    "docs = [doc for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 15848 documents\n",
      "500/15848\n",
      "1000/15848\n",
      "1500/15848\n",
      "2000/15848\n",
      "2500/15848\n",
      "3000/15848\n",
      "3500/15848\n",
      "4000/15848\n",
      "4500/15848\n",
      "5000/15848\n",
      "5500/15848\n",
      "6000/15848\n",
      "6500/15848\n",
      "7000/15848\n",
      "7500/15848\n",
      "8000/15848\n",
      "8500/15848\n",
      "9000/15848\n",
      "9500/15848\n",
      "10000/15848\n",
      "10500/15848\n",
      "11000/15848\n",
      "11500/15848\n",
      "12000/15848\n",
      "12500/15848\n",
      "13000/15848\n",
      "13500/15848\n",
      "14000/15848\n",
      "14500/15848\n",
      "15000/15848\n",
      "15500/15848\n"
     ]
    }
   ],
   "source": [
    "if len(docs) > len(old_lemmas):\n",
    "    new_docs = docs[len(old_lemmas) :]\n",
    "    docs_count = len(new_docs)\n",
    "    new_lemmas = []\n",
    "    print(f\"Tokenizing {docs_count} documents\")\n",
    "    i = 1\n",
    "    for doc in new_docs:\n",
    "        if i % 500 == 0:\n",
    "            print(f\"{i}/{docs_count}\")\n",
    "        new_lemmas.append(tokenize_doc(doc))\n",
    "        i += 1\n",
    "    lemmas += new_lemmas\n",
    "\n",
    "    with open(\"../data/lemmatized_desc.pkl\", \"wb\") as file:\n",
    "        pickle.dump(lemmas, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('team', 11074),\n",
       " ('data', 10622),\n",
       " ('sowie', 10308),\n",
       " ('management', 5454),\n",
       " ('projekt', 5422),\n",
       " ('unser', 5206),\n",
       " ('business', 5170),\n",
       " ('neu', 5003),\n",
       " ('kunde', 4969),\n",
       " ('unterstützen', 4792)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_in_one = [w for doc in lemmas for w in doc]\n",
    "bow = Counter(all_in_one)\n",
    "bow.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "\n",
    "dictionary = Dictionary(lemmas)\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in lemmas]\n",
    "tfidf = TfidfModel(bow_corpus)\n",
    "tfidf_corpus = tfidf[bow_corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction using TfidfVectorizer¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    ")\n",
    "\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(bow)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
